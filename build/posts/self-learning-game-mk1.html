<!doctype html>
<html><head><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/zenburn.min.css"/><link rel="stylesheet" type="text/css" href="../css/post.css"/><link rel="icon" type="image/png" href="../public/favicon.png"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>Self Learning Game Mark 1</title></head><body><header><a href="../" class="back">« Back</a><h1>Self Learning Game Mark 1</h1></header><time>August 29, 2017</time><article><p>Long have I&nbsp;been trying to&nbsp;make some time to&nbsp;properly use machine learning to&nbsp;play a&nbsp;game.</p>
<p>In&nbsp;the past, I&nbsp;tried teaching a&nbsp;neural network to&nbsp;play Mario. That did not go very well. <a href="http://richard.vanderdys.blog/posts/stupid-mario.html">Link</a></p>
<p>I&nbsp;believe there were too many inputs. I&nbsp;see what I&nbsp;was thinking, I&nbsp;wanted to&nbsp;give the&nbsp;network our entire visual input. But this is not entirely realistic. When we play games, we are not evaluating each pixel of&nbsp;each frame! We are using our incredible eyes to&nbsp;focus on&nbsp;certain parts of&nbsp;the screen, most notably where the&nbsp;player is and&nbsp;what objects are relatively close to&nbsp;him.</p>
<p>So, Round 2.</p>
<p>I&nbsp;made a&nbsp;simpler game where the&nbsp;goal is not to&nbsp;get hit by&nbsp;missiles.</p>
<p>Take a&nbsp;look:</p>
<iframe width="100%" height="900" src="https://dijs.github.io/ai-dodger/" allowfullscreen="allowfullscreen" frameborder="0"></iframe>
<p>So, I&nbsp;wanted to&nbsp;train a&nbsp;network to&nbsp;decide where to&nbsp;move next based on&nbsp;the current player's state.</p>
<p>After creating the&nbsp;actual game and&nbsp;playing it a&nbsp;bit, I&nbsp;generalized the&nbsp;code to&nbsp;accept an&nbsp;AI player.</p>
<p>I&nbsp;went through 3 different machine learning implementations before I&nbsp;actually got something that worked.</p>
<p>First, was a&nbsp;hand coded solution which tried to&nbsp;use a&nbsp;table with all possible
inputs and&nbsp;outcomes, and&nbsp;each cell was updated as&nbsp;to which action was best in&nbsp;that state. This solution did not seem to&nbsp;work at&nbsp;all, although it would be interesting to&nbsp;try again using what I&nbsp;have learned now.</p>
<p>I&nbsp;instead, tried to&nbsp;use a&nbsp;already built solution for&nbsp;neural networks. I&nbsp;had heard great things about ConvNetJS, so I&nbsp;implemented the&nbsp;<a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">Reinforcement Learning Brain</a> to&nbsp;be my&nbsp;actor. The&nbsp;result was not much better… At&nbsp;this point, I&nbsp;created the&nbsp;"Average Reward Over Time" visualization that you can see on&nbsp;the right of&nbsp;the game. These numbers should be growing as&nbsp;time passes, since the&nbsp;AI is supposed to&nbsp;get smarter. But they were not. So, I&nbsp;scrapped that library.</p>
<p>I&nbsp;went with another <a href="http://cs.stanford.edu/people/karpathy/reinforcejs/">library</a> that focused only on&nbsp;reinforcement learning. Same author. Since I&nbsp;had already generalized the&nbsp;code, implementing this new library was very straightforward. And&nbsp;almost immediately I&nbsp;saw actual learning. So, after cleaning up the&nbsp;code and&nbsp;putting some lipstick on&nbsp;the game itself, I&nbsp;called it a&nbsp;success.</p>
<p>Finally, an&nbsp;AI player that teaches itself to&nbsp;play a&nbsp;simple game!</p>
<p>For&nbsp;those who are interested, the&nbsp;training inputs I&nbsp;finalized on&nbsp;were simply an&nbsp;array of&nbsp;binary digits which signified whether there existed enemies relative to&nbsp;the players positions (in&nbsp;a line of&nbsp;sight fashion). This can be easily seen in&nbsp;the game. I&nbsp;have highlighted the&nbsp;inputs as&nbsp;green and&nbsp;red. The&nbsp;small size of&nbsp;the input really improved the&nbsp;learning rate of&nbsp;the AI player. The&nbsp;reward was simply based on&nbsp;whether or&nbsp;not the&nbsp;chosen state kept the&nbsp;player alive or&nbsp;not.</p>
<p>The&nbsp;<abbr>HUGE</abbr> difference here (between this and&nbsp;Mario) if you have not guessed already is that I&nbsp;have the&nbsp;game state, and&nbsp;not just the&nbsp;frame buffer to&nbsp;work with. I&nbsp;imagine I&nbsp;could use a&nbsp;similar solution to&nbsp;teach Mario to&nbsp;survive if I&nbsp;figured out a&nbsp;way to&nbsp;reduce a&nbsp;simplified game state.</p>
<p>Anyways, Thanks for&nbsp;reading if you made it this far!</p>
</article></body></html>